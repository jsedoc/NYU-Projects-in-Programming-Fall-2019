{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All material ©2019, Alex Siegman\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is a LOT of useful information onthe internet, and as data scientists you'll often need access to that information. \n",
    "\n",
    "### Unfortunatley, rarely is that information contained neatly in CSVs or even in tabular form. Rather, you have to really work to get what you need. \n",
    "\n",
    "### Lucky for us, there are some useful tools for \"scraping\" the web – in particular, one called BeautifulSoup (https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before we delve in, here's an example of the power of BeautifulSoup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the file attached is a simple csv containing 100 unique URLs from WSJ.com\n",
    "# the script in this cell allows us to find the word count for each article (stored in the article metadata) via the URL\n",
    "\n",
    "url_list = [] # create an empty list called 'url_list' where we will store all of the URL's in question\n",
    "\n",
    "word_count_list = [] # create an empty list called 'word_count_list' where we will store the word counts associated  \n",
    "                     # with each URL in our 'url_list'\n",
    "\n",
    "with open(\"URLS_for_WordCount.csv\", newline='') as csvfile:\n",
    "          # note that you will have to navigate to wherever it is you have stored your csv as a pathname\n",
    "        \n",
    "    reader = csv.DictReader(csvfile) # this allows us to map our information in each row to an OrderedDictionary \n",
    "                                     # for more on DictReader see https://docs.python.org/3/library/csv.html\n",
    "    \n",
    "    for row in reader: # for every row in our csv, aka, for every dictionary entry (which is composed of our URLs)...\n",
    "        \n",
    "        # NB: you can use \"print(row)\" here to see what our ordered dictionary looks like \n",
    "        \n",
    "        for k, v in row.items(): # for every key, value pair in our ordered dictionary...\n",
    "            \n",
    "            # NB: again, you can use \"print(k)\" or \"print(v)\" here to see what our key, value pairs look like \n",
    "\n",
    "            url_list.append(str(v)) # add the URL to our \"url_list\"\n",
    "            \n",
    "            r = requests.get(v) # for more on the requests library check out this tutorial from RealPython: \n",
    "                                # https://realpython.com/python-requests/\n",
    "            \n",
    "            soup = BeautifulSoup(r.text,'html') # we are going to turn that URL into 'soup', aka, we are going to be \n",
    "                                                # able to see it's metadata For more on BeautifulSoup, check out: \n",
    "                                                # https://www.crummy.com/software/BeautifulSoup/bs4/doc/\n",
    "            \n",
    "            wc1 = str(soup.find(\"meta\", property=\"article:word_count\")) # we want to find the word_count associated \n",
    "                                                                        # with each URL, found in the HTML that we \n",
    "                                                                        # have just \"souped\"\n",
    "            \n",
    "            wc2 = re.search('\\d+',wc1).group(0) # we use regular expressions to find the first number in the associated\n",
    "            # metadata, and store that. For more on regex see this great tutorial (not from me): https://regexr.com/\n",
    "        \n",
    "            word_count_list.append(wc2) # finally, we add (append) our word count to our \"word_count_list\"\n",
    "\n",
    "        break \n",
    "            \n",
    "print(word_count_list) # just to make sure everything works as planned\n",
    "print(url_list) # again, just to make sure everything works as planned      \n",
    "\n",
    "# the code below will create a new csv, called \"URL_for_WordCount_with_WordCounts.csv\" in our current directory\"\n",
    "# for more on csv.writer check out: https://docs.python.org/3/library/csv.html\n",
    "\n",
    "\"\"\" \n",
    "\n",
    "myData = url_list,word_count_list \n",
    "myFile = open('URL_for_WordCount_with_WordCounts.csv', 'w')  \n",
    "with myFile:  \n",
    "   writer = csv.writer(myFile)\n",
    "   writer.writerows(myData)\n",
    "   \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, back to BeautifulSoup basics:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.nytimes.com/\" # let's scrape the NYT homepage\n",
    "\n",
    " # the requests library is the easiest way to call to a URL; here we are using a GET command\n",
    "\n",
    " # we are going to take the result of that GET command and pass it through bs4\n",
    "\n",
    " # 'prettify' does exactly what you'd think – it prettifies the output of the print statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you're seeing above is the HTML for the NYT homepage. Let's continue with a few basics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # let's find the title of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # get a string version of the title \n",
    "\n",
    "# note that there are some encoding issues here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # find the parent of the title \n",
    "                       # this is exceptionally helpful when you're trying to parse an HTML tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # get the first <p> tag in the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # get the class of that <p> tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # find all 'a' tags on the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # find all 'a' on the page\n",
    "     # get the associated href (hyperlink) for each instance "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's important to know that BeautifulSoup transforms HTMl into a tree of Python objects. The most important objects to know are: \n",
    "\n",
    "1. Tag\n",
    "2. NavigableString\n",
    "3. BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A tag corresponds to an XML or HTML tag in the original document. For instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # you can easily access an attributes tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # or, you can search for a corresponding value as you would in a dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A string corresponds to a bit of text within a tag. You use the NavigableString class to access that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The BeautifulSoup object represents the document as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigating the Tree\n",
    "\n",
    "### The easiest way to navigate the parse tree is to call out the tag you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # let's just call out for the 'head' tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # or the 'title' tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can, of course, delve deeper into the parse tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # get the first <p> tag beneath the <body> tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that using a tag name as an attribute gets you only the first tag by that name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find all the tags, use something like find_all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As alluded to earlier, it's helpful to be able to navigate the tree step-by-step. A tag's children are available in a list called .contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can also iterate over a tag's children with the .children generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # simply pass in the string for the tag you're searching for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re # you can pass in regular expressions, too\n",
    "\n",
    " # find all tags whose names start with 'p'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # find all the tags whose names contain the letter 't'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # if you pass a list, bs4 will match against any item in that list "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering by CSS Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# note the class_, since class is a reserved word in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding\n",
    "\n",
    "### Last but not least, it's important to remember that any HTML or XML is written in a specific encoding (ASCII, UTF-8, et. cetera). But, bs4 turns that into Unicode, and sometimes it makes mistakes. \n",
    "\n",
    "### If you know the encoding ahead of time, specify it when you originally pass it in. For instance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup = BeautifulSoup(html, from_encoding=\"iso-8859-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### As you can see, there is a lot that can be done with BeautifulSoup! Just like anything else, the key is to know what can be done. Then, refer to the documentation. \n",
    "\n",
    "### Next week we'll take our BeautifulSoup skills and marry them with some Natural Language Processing and text mining capabilities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
