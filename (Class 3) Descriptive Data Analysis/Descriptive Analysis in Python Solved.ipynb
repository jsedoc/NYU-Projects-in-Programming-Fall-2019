{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Descriptive Analysis in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are going to use the NYC Vehicle Collisions 'accidents.csv' dataset from:\n",
    "https://data.cityofnewyork.us/Public-Safety/NYPD-Motor-Vehicle-Collisions-Crashes/h9gi-nx95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl 'https://data.cityofnewyork.us/api/views/h9gi-nx95/rows.csv?accessType=DOWNLOAD' -o accidents.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./accidents.csv\",low_memory=False)\n",
    "\n",
    "# low_memory means we want to read in our full data set \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes # let's check our datatypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It looks like our date and time columns are being read in as strings, not as datetimes, so let's adjust:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DATETIME'] = df['DATE'] + ' ' + df['TIME'] # let's create a new column called DATETIME...\n",
    "df.DATETIME = pd.to_datetime(df.DATETIME, format=\"%m/%d/%Y %H:%M\") # ...and convert it to datetime format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can also convert our original 'DATE' and 'TIME' columns to datetime format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TIME'] = pd.to_datetime(df.TIME, format=\"%H:%M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DATE'] = pd.to_datetime(df.DATE, format=\"%m/%d/%Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's create a new column called \"INJURY\" and another called \"DEATH\" that only holds a True value if there was at least one injury or death in the incident:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['INJURY'] = (df['NUMBER OF PERSONS INJURED']>0)\n",
    "df['DEATH'] = (df['NUMBER OF PERSONS KILLED']>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: What is the most common contributing factor to collisions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['CONTRIBUTING FACTOR VEHICLE 1'].value_counts()[1:10].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Break down the number of collisions by borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['BOROUGH'].value_counts().plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On a somewhat related note, let's see what sort of plotting we can do with this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(kind='scatter', x='LONGITUDE', y='LATITUDE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clearly there are some erronenous values here. I don't think there were any accidents in NYC at longitude 0, latitude 40 (especially because that is somewhere in Spain). So let's use what's known as a 'Mask' – a selection condition that only keeps the entries we determine to be valid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mask = (df.LATITUDE > 40) & (df.LATITUDE < 41) & (df.LONGITUDE < -72) & (df.LONGITUDE > -74.5)\n",
    "\n",
    "cleandf = df[clean_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf.plot(kind='scatter', x='LONGITUDE', y='LATITUDE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf.plot(kind='scatter', x='LONGITUDE', y='LATITUDE', figsize=(20, 15)) # increasing the figsize a bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Although cool, this isn't exactly helpful, because there are just so many datapoints. This is known as \"Overplotting.\"\n",
    "\n",
    "## To get around overplotting, we can sample our data by either specifying the number of points we want to keep, or the percentage of our data set that we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = cleandf.sample(n=10000) # keep 10,000 data points\n",
    "\n",
    "sample.plot(kind='scatter', x='LONGITUDE', y='LATITUDE', figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = cleandf.sample(frac=0.01) # keep 1% of the dataset\n",
    "\n",
    "sample.plot(kind='scatter', x='LONGITUDE', y='LATITUDE', figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another option is to change our marker size or marker transparency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf.plot(\n",
    "    kind='scatter', x='LONGITUDE', y='LATITUDE', figsize=(20, 15), s=0.5 ) # changing the marker size ('s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf.plot(\n",
    "    kind='scatter',\n",
    "    x='LONGITUDE',\n",
    "    y='LATITUDE',\n",
    "    figsize=(20, 15),\n",
    "    s=0.5,\n",
    "    alpha=0.05) # changing the marker transparency ('alpha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, back to working with datetimes, in particular, time series data (data that is obtained at successive times, often with equal intervals between them).\n",
    "\n",
    "Pandas has proven very successful as a tool for working with time series data, especially in the financial data analysis space. Using the [NumPy](http://www.numpy.org/) `datetime64` and `timedelta64` dtypes, pandas consolidated a large number of features from other Python libraries as well as created a tremendous amount of new functionality for manipulating time series data.\n",
    "\n",
    "In working with time series data, you will frequently seek to:\n",
    "\n",
    "* generate sequences of fixed-frequency dates and time spans\n",
    "* conform or convert time series to a particular frequency\n",
    "* compute \"relative\" dates based on various non-standard time increments (e.g. 5 business days before the last business day of the year) or \"roll\" dates forward or backward\n",
    "\n",
    "The following examples demonstrate how to work with time, date and datetime data using pandas.\n",
    "\n",
    "First, let's generate a list of datetime values for 3 neighboring days starting from `01/30/2017` with the interval equals to one hour. \n",
    "\n",
    "The [`date_range`](http://pandas.pydata.org/pandas-docs/version/0.19.1/generated/pandas.date_range.html) function can return such list in [`DatetimeIndex`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DatetimeIndex.html) format. \n",
    "\n",
    "We simply should define the left bound for generating dates `start` (`\"01/30/2017\"` in our case), the right bound for generating dates `end` or the amount of intervals `periods` (`3*24` in our case, because we are going to cover three days) and preferable frequency `freq` (`'H'` in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_range = pd.date_range(start='01/30/2017', periods=3*24, freq='H')\n",
    "\n",
    "print(\"Number of elements:\", len(example_range.values))\n",
    "example_range[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pd.to_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert a `Series` or list-like object of date-like objects e.g. strings, epochs, or a mixture, you can use the [`to_datetime`](http://pandas.pydata.org/pandas-docs/version/0.19.2/generated/pandas.to_datetime.html) function. When passed a `Series`, this returns a Series (with the same index), while a list-like is converted to a `DatetimeIndex`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_datetime(pd.Series(['Jul 31, 2009', '2010-12-10', None])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We're going to be looking some Daily minimum temperatures in Melbourne data from Kaggle: \n",
    "\n",
    "https://www.kaggle.com/paulbrabban/daily-minimum-temperatures-in-melbourne\n",
    "\n",
    "## You can download this dataset using the \"Download\" section. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "melbourne_temp = pd.read_csv(\"./daily-minimum-temperatures-in-melbourne.csv\", \n",
    "                             skiprows=1,  # let's us skip the header\n",
    "                             names=[\"date\", \"temp\"])  # let's name columns as we desire\n",
    "melbourne_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's drop that final row:\n",
    "\n",
    "melbourne_temp.drop(melbourne_temp.tail(1).index,inplace=True) \n",
    "\n",
    "melbourne_temp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_temp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_temp['temp'] = melbourne_temp['temp'].apply(pd.to_numeric, errors='Coerce')\n",
    "melbourne_temp['date'] = pd.to_datetime(melbourne_temp['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_temp.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "\n",
    "rcParams['figure.figsize'] = (16, 6) # change figsize \n",
    "\n",
    "melbourne_temp.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_temp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Find the minimum temperture for all of the recorded history included in this data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melbourne_temp.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Find the temperature for every day in January, 1981:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# melbourne_temp['1981-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Find the temperature for every day between Jan. 5, 1990 and Jan. 12, 1990:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melbourne_temp['1990-01-05':'1990-01-12']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Plot the temperature for every day in February, 1981:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melbourne_temp['1981-02'].plot()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Find the average temperature in February, 1981:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melbourne_temp['1981-02'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Find how many days the temperature was less than 2 degress across the whole data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Days with temperature less than 2 degrees:\", (melbourne_temp < 2).sum().values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "Resampling involves changing the frequency of your time series observations. \n",
    "\n",
    "- Upsampling means you increase the frequency of the samples (say, from minutes to seconds)\n",
    "- Downsampling means you decrease the frequency of the samples (say, from months to days)\n",
    "\n",
    "## Imagine we are trying to determine the average weekly temperture from our Melbourne set, for instance. In that case, we would need to upsample the information from daily to weekly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_temp.resample('W').mean().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melbourne_temp.resample('A').min() # 'A' means year-end frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling is also a fast way to smooth in some essence the time series. For instance, the time series of monthly averages has much less peaks or outliers and allows tracking the periodicity. \n",
    "\n",
    "## One way to smooth a time series is to calculate a rolling average – the average value of the current point and `N` previous points. It can be calcualted using [`rolling`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html) pandas's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = melbourne_temp.plot(alpha=0.25) # draw initial time series and make it transparent\n",
    "\n",
    "melbourne_temp.resample('M').mean().plot(ax=ax) # draw montly average values\n",
    "\n",
    "melbourne_temp.rolling(25).mean().plot(ax=ax) # draw roling average that takes into account 25 points\n",
    "\n",
    "ax.legend([\"daily data\", \"montly average\", \"roling mean\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you set the hourly frequency in the [`resample()`](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.resample.html) function, then `NaN` values will be created, because there are no more tiny distribution of the temperature.\n",
    "\n",
    "## This is upsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upsampled = melbourne_temp.resample('H').mean()\n",
    "upsampled.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But we can interpolate the missing values at this new frequency.\n",
    "\n",
    "## The `Series` pandas's object provides the [`interpolate`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.interpolate.html) function to interpolate missing values. A good starting point is to use a `linear interpolation`. This draws a straight line between available data and fills in values at the chosen frequency from this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "interpolated = upsampled.interpolate(method='linear')\n",
    "interpolated.head(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
